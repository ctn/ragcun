{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LeJEPA on TPU (Google Colab)\n",
    "\n",
    "âš ï¸ **Note**: TPU training is more complex than GPU. Use GPU unless you need TPU's speed for large-scale training.\n",
    "\n",
    "**When to use TPU:**\n",
    "- Training on 50K+ samples\n",
    "- Batch size 128+\n",
    "- GPUs unavailable\n",
    "\n",
    "**Limitations:**\n",
    "- FAISS doesn't work on TPU (uses CPU fallback)\n",
    "- Some PyTorch ops not supported\n",
    "- More complex debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check TPU availability\n",
    "import os\n",
    "\n",
    "# Set TPU environment\n",
    "if 'TPU_NAME' in os.environ:\n",
    "    print(f\"âœ… TPU detected: {os.environ['TPU_NAME']}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No TPU found. Go to Runtime â†’ Change runtime type â†’ TPU\")\n",
    "    raise RuntimeError(\"Please enable TPU in Colab settings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TPU-specific packages\n",
    "!pip install -q cloud-tpu-client\n",
    "!pip install -q torch-xla>=2.0.0\n",
    "\n",
    "# Install other dependencies\n",
    "!pip install -q transformers>=4.45.0 sentence-transformers>=3.0.0 datasets\n",
    "!pip install -q accelerate\n",
    "!pip install -q lejepa || pip install -q git+https://github.com/rbalestr-lab/lejepa.git\n",
    "\n",
    "print(\"âœ… Packages installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports (TPU-specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import lejepa\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# TPU-specific imports\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "\n",
    "print(\"âœ… Imports successful\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"XLA: {torch_xla.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Definition (Same as GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsotropicGaussianEncoder(nn.Module):\n",
    "    \"\"\"Same model as GPU version - TPU compatible\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dim=512, freeze_early_layers=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        print(\"Loading EmbeddingGemma-300M...\")\n",
    "        self.base = SentenceTransformer(\n",
    "            'google/embeddinggemma-300m',\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(768, 768 * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(768 * 2, output_dim)\n",
    "        )\n",
    "        \n",
    "        for param in self.base.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        if freeze_early_layers:\n",
    "            frozen = 0\n",
    "            for name, param in self.base.named_parameters():\n",
    "                if any(f'encoder.layer.{i}.' in name for i in range(4)):\n",
    "                    param.requires_grad = False\n",
    "                    frozen += 1\n",
    "            print(f\"Froze {frozen} parameters\")\n",
    "        \n",
    "        total = sum(p.numel() for p in self.parameters())\n",
    "        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f\"Total: {total:,}, Trainable: {trainable:,}\")\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "    def encode(self, texts, batch_size=32, show_progress=False):\n",
    "        base_emb = self.base.encode(\n",
    "            texts,\n",
    "            batch_size=batch_size,\n",
    "            convert_to_tensor=True,\n",
    "            show_progress_bar=show_progress,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "        gaussian_emb = self.projection(base_emb)\n",
    "        return gaussian_emb\n",
    "    \n",
    "    def forward(self, texts):\n",
    "        return self.encode(texts, show_progress=False)\n",
    "\n",
    "print(\"âœ… Model class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Data (Same as GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading training data...\")\n",
    "dataset = load_dataset(\n",
    "    'sentence-transformers/msmarco-hard-negatives',\n",
    "    'triplet',\n",
    "    split='train',\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "num_samples = 5000\n",
    "data = []\n",
    "\n",
    "for i, example in enumerate(dataset):\n",
    "    if i >= num_samples:\n",
    "        break\n",
    "    data.append({\n",
    "        'query': example['query'],\n",
    "        'positive': example['positive'],\n",
    "        'negative': example['negative']\n",
    "    })\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(f\"  Loaded {i + 1}...\")\n",
    "\n",
    "train_size = int(0.9 * len(data))\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:]\n",
    "\n",
    "print(f\"âœ… Train: {len(train_data)}, Val: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'queries': [item['query'] for item in batch],\n",
    "        'positives': [item['positive'] for item in batch],\n",
    "        'negatives': [item['negative'] for item in batch]\n",
    "    }\n",
    "\n",
    "# TPU works best with larger batches\n",
    "batch_size = 128  # Increase for TPU (GPU: 16)\n",
    "\n",
    "train_dataset = TripletDataset(train_data)\n",
    "val_dataset = TripletDataset(val_data)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0  # TPU doesn't need workers\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"âœ… Batch size: {batch_size} (larger for TPU efficiency)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TPU-Specific Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_tpu():\n",
    "    \"\"\"Main training function for TPU\"\"\"\n",
    "    \n",
    "    # Get TPU device\n",
    "    device = xm.xla_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = IsotropicGaussianEncoder(output_dim=512).to(device)\n",
    "    \n",
    "    # LeJEPA loss\n",
    "    sigreg = lejepa.multivariate.SlicingUnivariateTest(\n",
    "        univariate_test=lejepa.univariate.EppsPulley(num_points=17),\n",
    "        num_slices=1024\n",
    "    ).to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=1e-5,\n",
    "        weight_decay=0.05\n",
    "    )\n",
    "    \n",
    "    # Scheduler\n",
    "    num_epochs = 5\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=num_epochs * len(train_loader)\n",
    "    )\n",
    "    \n",
    "    lambda_contrastive = 1.0\n",
    "    lambda_isotropy = 0.01\n",
    "    \n",
    "    Path('checkpoints').mkdir(exist_ok=True)\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Starting TPU Training!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        \n",
    "        # TPU-specific: wrap dataloader\n",
    "        para_loader = pl.ParallelLoader(train_loader, [device])\n",
    "        train_loader_tpu = para_loader.per_device_loader(device)\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader_tpu, desc=\"Training\")):\n",
    "            # Encode triplets\n",
    "            q_emb = model(batch['queries'])\n",
    "            pos_emb = model(batch['positives'])\n",
    "            neg_emb = model(batch['negatives'])\n",
    "            \n",
    "            # Losses\n",
    "            pos_dist = torch.norm(q_emb - pos_emb, dim=1)\n",
    "            neg_dist = torch.norm(q_emb - neg_emb, dim=1)\n",
    "            contrastive_loss = torch.relu(pos_dist - neg_dist + 1.0).mean()\n",
    "            \n",
    "            all_emb = torch.cat([q_emb, pos_emb, neg_emb], dim=0)\n",
    "            isotropy_loss = sigreg(all_emb)\n",
    "            \n",
    "            loss = lambda_contrastive * contrastive_loss + lambda_isotropy * isotropy_loss\n",
    "            \n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # TPU-specific: sync gradients\n",
    "            xm.optimizer_step(optimizer)\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                xm.master_print(f\"  Batch {batch_idx}: loss={loss.item():.4f}\")\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        xm.master_print(f\"\\nTrain Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        para_loader_val = pl.ParallelLoader(val_loader, [device])\n",
    "        val_loader_tpu = para_loader_val.per_device_loader(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader_tpu, desc=\"Validation\"):\n",
    "                q_emb = model(batch['queries'])\n",
    "                pos_emb = model(batch['positives'])\n",
    "                neg_emb = model(batch['negatives'])\n",
    "                \n",
    "                pos_dist = torch.norm(q_emb - pos_emb, dim=1)\n",
    "                neg_dist = torch.norm(q_emb - neg_emb, dim=1)\n",
    "                contrastive_loss = torch.relu(pos_dist - neg_dist + 1.0).mean()\n",
    "                \n",
    "                all_emb = torch.cat([q_emb, pos_emb, neg_emb], dim=0)\n",
    "                isotropy_loss = sigreg(all_emb)\n",
    "                \n",
    "                loss = lambda_contrastive * contrastive_loss + lambda_isotropy * isotropy_loss\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        xm.master_print(f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model (only on master TPU core)\n",
    "        if xm.is_master_ordinal() and avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            # Move to CPU before saving\n",
    "            model_cpu = model.cpu()\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model_cpu.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, 'checkpoints/best_model.pt')\n",
    "            model.to(device)  # Move back to TPU\n",
    "            xm.master_print(\"  â­ Saved best model!\")\n",
    "    \n",
    "    xm.master_print(\"\\nâœ… Training complete!\")\n",
    "\n",
    "print(\"âœ… Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Training on TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "# Note: TPU uses all 8 cores by default\n",
    "train_on_tpu()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training finished! Model saved to checkpoints/best_model.pt\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Save final model\n",
    "final_path = 'gaussian_embeddinggemma_final.pt'\n",
    "\n",
    "# Load checkpoint and save just the model state dict\n",
    "checkpoint = torch.load('checkpoints/best_model.pt', map_location='cpu')\n",
    "torch.save(checkpoint['model_state_dict'], final_path)\n",
    "\n",
    "print(f\"âœ… Model saved: {final_path}\")\n",
    "\n",
    "# Download\n",
    "files.download(final_path)\n",
    "\n",
    "print(\"\\nðŸ“¥ Download complete!\")\n",
    "print(\"Next: Save to /Users/ctn/src/ctn/ragcun/data/embeddings/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ TPU Training Notes\n",
    "\n",
    "### **Key Differences from GPU:**\n",
    "\n",
    "1. **Batch Size:**\n",
    "   - GPU: 16-32 optimal\n",
    "   - TPU: 128+ optimal (TPUs love large batches!)\n",
    "\n",
    "2. **DataLoader:**\n",
    "   - Must use `ParallelLoader` wrapper\n",
    "   - `num_workers=0` (TPU handles parallelism)\n",
    "\n",
    "3. **Optimizer Step:**\n",
    "   - Use `xm.optimizer_step()` instead of `optimizer.step()`\n",
    "   - Syncs gradients across TPU cores\n",
    "\n",
    "4. **Device:**\n",
    "   - `device = xm.xla_device()` instead of `cuda`\n",
    "\n",
    "5. **Saving:**\n",
    "   - Move model to CPU before saving\n",
    "   - Only save on master ordinal\n",
    "\n",
    "### **Performance:**\n",
    "\n",
    "- **Speed:** ~2-3x faster than T4 GPU for large batches\n",
    "- **Cost:** Free in Colab (same as GPU)\n",
    "- **Complexity:** Higher (more code, harder debugging)\n",
    "\n",
    "### **Recommendation:**\n",
    "\n",
    "Use **GPU (T4/V100)** unless:\n",
    "- âœ… Training on 50K+ samples\n",
    "- âœ… Using batch size 128+\n",
    "- âœ… GPUs are unavailable\n",
    "\n",
    "For this project (5K samples, batch 16-32): **GPU is simpler and sufficient!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
