{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LeJEPA Isotropic Gaussian Embeddings\n",
    "\n",
    "This notebook trains EmbeddingGemma-300M with LeJEPA loss to produce isotropic Gaussian embeddings for improved RAG retrieval.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ctn/ragcun/blob/main/notebooks/lejepa_training.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup GPU and Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers>=4.45.0 sentence-transformers>=3.0.0 datasets\n",
    "!pip install -q faiss-gpu accelerate\n",
    "!pip install -q lejepa || pip install -q git+https://github.com/rbalestr-lab/lejepa.git\n",
    "\n",
    "print(\"âœ… All packages installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import lejepa\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"âœ… Imports successful!\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model: Gaussian EmbeddingGemma\n",
    "\n",
    "This model wraps EmbeddingGemma-300M and adds a projection layer to produce **unnormalized isotropic Gaussian embeddings**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianEmbeddingGemma(nn.Module):\n",
    "    \"\"\"\n",
    "    EmbeddingGemma with LeJEPA-trained projection to isotropic Gaussian space.\n",
    "    \n",
    "    Key features:\n",
    "    - Starts with EmbeddingGemma-300M (state-of-the-art for RAG)\n",
    "    - Projects to unnormalized Gaussian space (NO L2 normalization)\n",
    "    - Trained with LeJEPA SIGReg loss for isotropy\n",
    "    - Uses Euclidean distance for retrieval (not cosine similarity)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dim=512, freeze_early_layers=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        print(\"Loading EmbeddingGemma-300M...\")\n",
    "        self.base = SentenceTransformer(\n",
    "            'google/embeddinggemma-300m', \n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Projection: 768 (normalized) â†’ output_dim (Gaussian)\n",
    "        # NO normalization layers!\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(768, 768 * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(768 * 2, output_dim)\n",
    "        )\n",
    "        \n",
    "        # Make base trainable\n",
    "        for param in self.base.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # Optionally freeze early layers\n",
    "        if freeze_early_layers:\n",
    "            frozen = 0\n",
    "            for name, param in self.base.named_parameters():\n",
    "                if any(f'encoder.layer.{i}.' in name for i in range(4)):\n",
    "                    param.requires_grad = False\n",
    "                    frozen += 1\n",
    "            print(f\"Froze {frozen} parameters in early layers\")\n",
    "        \n",
    "        total = sum(p.numel() for p in self.parameters())\n",
    "        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f\"Total params: {total:,}\")\n",
    "        print(f\"Trainable: {trainable:,} ({100*trainable/total:.1f}%)\")\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "    def encode(self, texts, batch_size=32, show_progress=False):\n",
    "        \"\"\"Encode texts to Gaussian embeddings (NOT normalized)\"\"\"\n",
    "        base_emb = self.base.encode(\n",
    "            texts,\n",
    "            batch_size=batch_size,\n",
    "            convert_to_tensor=True,\n",
    "            show_progress_bar=show_progress,\n",
    "            normalize_embeddings=True  # Base does L2 norm\n",
    "        )\n",
    "        \n",
    "        # Project to Gaussian space (undoes normalization)\n",
    "        gaussian_emb = self.projection(base_emb)\n",
    "        return gaussian_emb\n",
    "    \n",
    "    def forward(self, texts):\n",
    "        return self.encode(texts, show_progress=False)\n",
    "\n",
    "print(\"âœ… Model class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load MS MARCO dataset (query-positive-negative triplets)\n",
    "print(\"Loading training data...\")\n",
    "dataset = load_dataset(\n",
    "    'sentence-transformers/msmarco-hard-negatives',\n",
    "    'triplet',\n",
    "    split='train',\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Take 5000 samples for quick training\n",
    "num_samples = 5000\n",
    "data = []\n",
    "\n",
    "for i, example in enumerate(dataset):\n",
    "    if i >= num_samples:\n",
    "        break\n",
    "    data.append({\n",
    "        'query': example['query'],\n",
    "        'positive': example['positive'],\n",
    "        'negative': example['negative']\n",
    "    })\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(f\"  Loaded {i + 1} examples...\")\n",
    "\n",
    "# Train/val split\n",
    "train_size = int(0.9 * len(data))\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:]\n",
    "\n",
    "print(f\"âœ… Train: {len(train_data)}, Val: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'queries': [item['query'] for item in batch],\n",
    "        'positives': [item['positive'] for item in batch],\n",
    "        'negatives': [item['negative'] for item in batch]\n",
    "    }\n",
    "\n",
    "batch_size = 16  # Adjust based on GPU (T4: 8-16, A100: 32-64)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TripletDataset(train_data),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    TripletDataset(val_data),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"âœ… DataLoaders: {len(train_loader)} train, {len(val_loader)} val batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Model and LeJEPA Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GaussianEmbeddingGemma(output_dim=512).to(device)\n",
    "\n",
    "# LeJEPA loss (SIGReg)\n",
    "print(\"\\nInitializing LeJEPA SIGReg...\")\n",
    "sigreg = lejepa.multivariate.SlicingUnivariateTest(\n",
    "    univariate_test=lejepa.univariate.EppsPulley(num_points=17),\n",
    "    num_slices=1024\n",
    ").to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-5,\n",
    "    weight_decay=0.05\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "num_epochs = 5\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=num_epochs * len(train_loader)\n",
    ")\n",
    "\n",
    "# Loss weights (from LeJEPA paper)\n",
    "lambda_contrastive = 1.0\n",
    "lambda_isotropy = 0.01\n",
    "\n",
    "print(\"âœ… Training setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_isotropy(embeddings, verbose=True):\n",
    "    \"\"\"Verify embeddings are isotropic Gaussian N(0, I)\"\"\"\n",
    "    embeddings = embeddings.detach().cpu()\n",
    "    \n",
    "    mean = embeddings.mean(dim=0)\n",
    "    centered = embeddings - mean\n",
    "    cov = (centered.T @ centered) / (embeddings.shape[0] - 1)\n",
    "    \n",
    "    mean_norm = torch.norm(mean).item()\n",
    "    cov_error = torch.norm(cov - torch.eye(cov.shape[0]), p='fro').item()\n",
    "    diag_mean = torch.diag(cov).mean().item()\n",
    "    \n",
    "    off_diag = cov.clone()\n",
    "    off_diag.fill_diagonal_(0)\n",
    "    off_diag_mean = off_diag.abs().mean().item()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Mean norm: {mean_norm:.4f} (want ~0)\")\n",
    "        print(f\"  Cov error: {cov_error:.4f} (want <5)\")\n",
    "        print(f\"  Diag mean: {diag_mean:.4f} (want ~1)\")\n",
    "        print(f\"  Off-diag: {off_diag_mean:.4f} (want ~0)\")\n",
    "    \n",
    "    return {\n",
    "        'mean_norm': mean_norm,\n",
    "        'cov_error': cov_error,\n",
    "        'is_isotropic': mean_norm < 0.5 and cov_error < 10.0\n",
    "    }\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, path)\n",
    "    print(f\"âœ… Saved: {path}\")\n",
    "\n",
    "print(\"âœ… Utilities defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_contrastive = 0\n",
    "    total_isotropy = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "        # Encode triplets\n",
    "        q_emb = model(batch['queries'])\n",
    "        pos_emb = model(batch['positives'])\n",
    "        neg_emb = model(batch['negatives'])\n",
    "        \n",
    "        # Euclidean contrastive loss\n",
    "        pos_dist = torch.norm(q_emb - pos_emb, dim=1)\n",
    "        neg_dist = torch.norm(q_emb - neg_emb, dim=1)\n",
    "        contrastive_loss = torch.relu(pos_dist - neg_dist + 1.0).mean()\n",
    "        \n",
    "        # LeJEPA isotropy loss\n",
    "        all_emb = torch.cat([q_emb, pos_emb, neg_emb], dim=0)\n",
    "        isotropy_loss = sigreg(all_emb)\n",
    "        \n",
    "        # Combined\n",
    "        loss = lambda_contrastive * contrastive_loss + lambda_isotropy * isotropy_loss\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_contrastive += contrastive_loss.item()\n",
    "        total_isotropy += isotropy_loss.item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'contr': f'{contrastive_loss.item():.4f}',\n",
    "            'iso': f'{isotropy_loss.item():.4f}'\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(loader),\n",
    "        'contrastive': total_contrastive / len(loader),\n",
    "        'isotropy': total_isotropy / len(loader)\n",
    "    }\n",
    "\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validating\"):\n",
    "            q_emb = model(batch['queries'])\n",
    "            pos_emb = model(batch['positives'])\n",
    "            neg_emb = model(batch['negatives'])\n",
    "            \n",
    "            pos_dist = torch.norm(q_emb - pos_emb, dim=1)\n",
    "            neg_dist = torch.norm(q_emb - neg_emb, dim=1)\n",
    "            contrastive_loss = torch.relu(pos_dist - neg_dist + 1.0).mean()\n",
    "            \n",
    "            all_emb = torch.cat([q_emb, pos_emb, neg_emb], dim=0)\n",
    "            isotropy_loss = sigreg(all_emb)\n",
    "            \n",
    "            loss = lambda_contrastive * contrastive_loss + lambda_isotropy * isotropy_loss\n",
    "            total_loss += loss.item()\n",
    "            all_embeddings.append(all_emb)\n",
    "    \n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    print(\"\\nIsotropy check:\")\n",
    "    isotropy_metrics = check_isotropy(all_embeddings)\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(loader),\n",
    "        'isotropy_metrics': isotropy_metrics\n",
    "    }\n",
    "\n",
    "print(\"âœ… Training functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('checkpoints').mkdir(exist_ok=True)\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Starting Training!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Train\n",
    "    train_metrics = train_epoch(model, train_loader, optimizer, scheduler, epoch)\n",
    "    print(f\"\\nTrain Loss: {train_metrics['loss']:.4f}\")\n",
    "    \n",
    "    # Validate\n",
    "    val_metrics = validate(model, val_loader)\n",
    "    print(f\"Val Loss: {val_metrics['loss']:.4f}\")\n",
    "    print(f\"Isotropic: {val_metrics['isotropy_metrics']['is_isotropic']}\")\n",
    "    \n",
    "    # Save best\n",
    "    if val_metrics['loss'] < best_val_loss:\n",
    "        best_val_loss = val_metrics['loss']\n",
    "        save_checkpoint(model, optimizer, epoch, 'checkpoints/best_model.pt')\n",
    "        print(\"  â­ New best model!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Training complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load('checkpoints/best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Test\n",
    "test_texts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Machine learning is a branch of AI\",\n",
    "    \"How to cook pasta\",\n",
    "    \"Python programming tutorial\"\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(test_texts)\n",
    "    print(f\"\\nShape: {embeddings.shape}\")\n",
    "    print(f\"Mean norm: {embeddings.norm(dim=1).mean():.4f}\")\n",
    "    print(f\"Std norm: {embeddings.norm(dim=1).std():.4f}\")\n",
    "\n",
    "check_isotropy(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save for RAGCUN\n",
    "\n",
    "Save the trained model to use in the RAGCUN retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_path = 'gaussian_embeddinggemma_final.pt'\n",
    "torch.save(model.state_dict(), final_path)\n",
    "print(f\"âœ… Model saved: {final_path}\")\n",
    "\n",
    "# Download to local machine\n",
    "from google.colab import files\n",
    "files.download(final_path)\n",
    "\n",
    "print(\"\\nðŸ“¥ Download complete!\")\n",
    "print(\"Next step: Add this model to /Users/ctn/src/ctn/ragcun/data/embeddings/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
