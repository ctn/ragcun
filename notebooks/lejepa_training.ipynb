{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yNOC4EX6hMt"
      },
      "source": [
        "# Train LeJEPA Isotropic Gaussian Embeddings\n",
        "\n",
        "This notebook trains EmbeddingGemma-300M with LeJEPA loss to produce isotropic Gaussian embeddings for improved RAG retrieval.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ctn/ragcun/blob/main/notebooks/lejepa_training.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiX3Y5gR6hMu"
      },
      "source": [
        "## 1. Setup GPU and Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QBNhsI56hMu",
        "outputId": "a7c72060-be97-488d-caa3-97902e951500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 14 08:40:36 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRketRwp6hMu",
        "outputId": "74c85760-3e67-4fc5-f45f-fe098a920d03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ… All packages installed!\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers>=4.45.0 sentence-transformers>=3.0.0 datasets\n",
        "!pip install -q faiss-cpu accelerate # Changed faiss-gpu to faiss-cpu\n",
        "!pip install -q lejepa || pip install -q git+https://github.com/rbalestr-lab/lejepa.git\n",
        "\n",
        "print(\"âœ… All packages installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le7WLtAz6hMv"
      },
      "source": [
        "## 2. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J7RctPr6hMv",
        "outputId": "011f6fa5-5cbd-406b-abf5-72d72e21a43f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Imports successful!\n",
            "PyTorch: 2.8.0+cu126\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import lejepa\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"âœ… Imports successful!\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq_w0w3P6hMv"
      },
      "source": [
        "## 3. Model: Gaussian EmbeddingGemma\n",
        "\n",
        "This model wraps EmbeddingGemma-300M and adds a projection layer to produce **unnormalized isotropic Gaussian embeddings**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyBbiYDm6hMw",
        "outputId": "98f2ae48-4858-46de-858e-bb2554747748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model class defined!\n"
          ]
        }
      ],
      "source": [
        "class GaussianEmbeddingGemma(nn.Module):\n",
        "    \"\"\"\n",
        "    EmbeddingGemma with LeJEPA-trained projection to isotropic Gaussian space.\n",
        "\n",
        "    Key features:\n",
        "    - Starts with EmbeddingGemma-300M (state-of-the-art for RAG)\n",
        "    - Projects to unnormalized Gaussian space (NO L2 normalization)\n",
        "    - Trained with LeJEPA SIGReg loss for isotropy\n",
        "    - Uses Euclidean distance for retrieval (not cosine similarity)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dim=512, freeze_early_layers=True):\n",
        "        super().__init__()\n",
        "\n",
        "        print(\"Loading EmbeddingGemma-300M...\")\n",
        "        self.base = SentenceTransformer(\n",
        "            'google/embeddinggemma-300m',\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Projection: 768 (normalized) â†’ output_dim (Gaussian)\n",
        "        # NO normalization layers!\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(768, 768 * 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768 * 2, output_dim)\n",
        "        )\n",
        "\n",
        "        # Make base trainable\n",
        "        for param in self.base.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # Optionally freeze early layers\n",
        "        if freeze_early_layers:\n",
        "            frozen = 0\n",
        "            for name, param in self.base.named_parameters():\n",
        "                if any(f'encoder.layer.{i}.' in name for i in range(4)):\n",
        "                    param.requires_grad = False\n",
        "                    frozen += 1\n",
        "            print(f\"Froze {frozen} parameters in early layers\")\n",
        "\n",
        "        total = sum(p.numel() for p in self.parameters())\n",
        "        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        print(f\"Total params: {total:,}\")\n",
        "        print(f\"Trainable: {trainable:,} ({100*trainable/total:.1f}%)\")\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def encode(self, texts, batch_size=32, show_progress=False):\n",
        "        \"\"\"Encode texts to Gaussian embeddings (NOT normalized)\"\"\"\n",
        "        base_emb = self.base.encode(\n",
        "            texts,\n",
        "            batch_size=batch_size,\n",
        "            convert_to_tensor=True,\n",
        "            show_progress_bar=show_progress,\n",
        "            normalize_embeddings=True  # Base does L2 norm\n",
        "        )\n",
        "\n",
        "        # Project to Gaussian space (undoes normalization)\n",
        "        gaussian_emb = self.projection(base_emb)\n",
        "        return gaussian_emb\n",
        "\n",
        "    def forward(self, texts):\n",
        "        return self.encode(texts, show_progress=False)\n",
        "\n",
        "print(\"âœ… Model class defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IEZtUE86hMw"
      },
      "source": [
        "## 4. Load Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "ewS5VKx76hMw",
        "outputId": "57270ca7-d645-4b0b-a711-5e5efcb9ec81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training data...\n",
            "Loading MS MARCO queries from Tevatron. This may take a moment and consume memory...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "BuilderConfig 'queries' not found. Available: ['default']",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-682440180.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading MS MARCO queries from Tevatron. This may take a moment and consume memory...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load queries from Tevatron/msmarco-passage (config 'queries') for consistency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtevatron_queries_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tevatron/msmarco-passage'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'queries'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mquery_id_to_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'qid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtevatron_queries_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Populating query map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loaded {len(query_id_to_text)} MS MARCO queries.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   1393\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[0mbuilder_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset_builder_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[0;31m# Instantiate the dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m     builder_instance: DatasetBuilder = builder_cls(\n\u001b[0m\u001b[1;32m   1167\u001b[0m         \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, repo_id, data_files, data_dir, storage_options, writer_batch_size, **config_kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0mconfig_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data_dir\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         self.config, self.config_id = self._create_builder_config(\n\u001b[0m\u001b[1;32m    344\u001b[0m             \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0mcustom_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_create_builder_config\u001b[0;34m(self, config_name, custom_features, **config_kwargs)\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0mbuilder_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuilder_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBUILDER_CONFIGS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    540\u001b[0m                     \u001b[0;34mf\"BuilderConfig '{config_name}' not found. Available: {list(self.builder_configs.keys())}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m                 )\n",
            "\u001b[0;31mValueError\u001b[0m: BuilderConfig 'queries' not found. Available: ['default']"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm # Import tqdm for progress bar\n",
        "\n",
        "# Load MS MARCO dataset (query-positive-negative triplets)\n",
        "print(\"Loading training data...\")\n",
        "\n",
        "# --- NEW: Load MS MARCO queries and passages for text mapping ---\n",
        "print(\"Loading MS MARCO queries from Tevatron. This may take a moment and consume memory...\")\n",
        "# Load queries from Tevatron/msmarco-passage (config 'queries') for consistency\n",
        "tevatron_queries_dataset = load_dataset('Tevatron/msmarco-passage', 'queries', split='train')\n",
        "query_id_to_text = {str(example['qid']): example['query'] for example in tqdm(tevatron_queries_dataset, desc=\"Populating query map\")}\n",
        "print(f\"Loaded {len(query_id_to_text)} MS MARCO queries.\")\n",
        "\n",
        "print(\"Loading full MS MARCO passage collection from Tevatron. This may take a moment and consume significant memory...\")\n",
        "# Load passages from Tevatron/msmarco-passage (config 'v1')\n",
        "passage_collection_dataset = load_dataset('Tevatron/msmarco-passage', 'v1', split='train')\n",
        "passage_id_to_text = {str(example['pid']): example['text'] for example in tqdm(passage_collection_dataset, desc=\"Populating passage map\")}\n",
        "print(f\"Loaded {len(passage_id_to_text)} MS MARCO passages.\")\n",
        "# --- END NEW ---\n",
        "\n",
        "# Load MS MARCO hard negatives (query-positive-negative triplets of IDs)\n",
        "# This dataset is loaded in streaming mode as we only take a subset\n",
        "hard_negatives_dataset = load_dataset(\n",
        "    'sentence-transformers/msmarco-hard-negatives',\n",
        "    'default', # Changed 'triplet' to 'default' in previous turn\n",
        "    split='train',\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "# Take 5000 samples for quick training\n",
        "num_samples = 5000\n",
        "data = []\n",
        "collected_triplets = 0\n",
        "\n",
        "# --- MODIFIED: Use mappings to get actual texts ---\n",
        "for i, example in enumerate(hard_negatives_dataset):\n",
        "    if collected_triplets >= num_samples:\n",
        "        break\n",
        "\n",
        "    # Get IDs from the hard_negatives_dataset (based on kernel state example)\n",
        "    query_id = str(example['qid'])\n",
        "    positive_id = str(example['pos'][0]) # Assuming 'pos' is a list and we take the first\n",
        "\n",
        "    # Assuming 'neg' is a dict and we take the first from 'bm25' key\n",
        "    negative_id = None\n",
        "    if 'bm25' in example['neg'] and example['neg']['bm25']:\n",
        "        negative_id = str(example['neg']['bm25'][0])\n",
        "\n",
        "    # Lookup texts using the maps\n",
        "    query_text = query_id_to_text.get(query_id)\n",
        "    positive_text = passage_id_to_text.get(positive_id)\n",
        "    negative_text = passage_id_to_text.get(negative_id)\n",
        "\n",
        "    if query_text and positive_text and negative_text: # Only add if all texts are found\n",
        "        data.append({\n",
        "            'query': query_text,\n",
        "            'positive': positive_text,\n",
        "            'negative': negative_text\n",
        "        })\n",
        "        collected_triplets += 1\n",
        "\n",
        "    if (i + 1) % 1000 == 0:\n",
        "        print(f\"  Processed {i + 1} hard negative examples from stream, collected {collected_triplets} full triplets...\")\n",
        "\n",
        "# Train/val split\n",
        "train_size = int(0.9 * len(data))\n",
        "train_data = data[:train_size]\n",
        "val_data = data[train_size:]\n",
        "\n",
        "print(f\"âœ… Train: {len(train_data)}, Val: {len(val_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhioXXXu6hMw"
      },
      "source": [
        "## 5. Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "e9e-Png36hMw",
        "outputId": "8713710c-bace-463a-c2bb-53e1267905e1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3139456520.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m train_loader = DataLoader(\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mTripletDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TripletDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'queries': [item['query'] for item in batch],\n",
        "        'positives': [item['positive'] for item in batch],\n",
        "        'negatives': [item['negative'] for item in batch]\n",
        "    }\n",
        "\n",
        "batch_size = 16  # Adjust based on GPU (T4: 8-16, A100: 32-64)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    TripletDataset(train_data),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    TripletDataset(val_data),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "print(f\"âœ… DataLoaders: {len(train_loader)} train, {len(val_loader)} val batches\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EXSn9El6hMx"
      },
      "source": [
        "## 6. Initialize Model and LeJEPA Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGG7Erxf6hMx"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GaussianEmbeddingGemma(output_dim=512).to(device)\n",
        "\n",
        "# LeJEPA loss (SIGReg)\n",
        "print(\"\\nInitializing LeJEPA SIGReg...\")\n",
        "sigreg = lejepa.multivariate.SlicingUnivariateTest(\n",
        "    univariate_test=lejepa.univariate.EppsPulley(num_points=17),\n",
        "    num_slices=1024\n",
        ").to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=1e-5,\n",
        "    weight_decay=0.05\n",
        ")\n",
        "\n",
        "# Scheduler\n",
        "num_epochs = 5\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer,\n",
        "    T_max=num_epochs * len(train_loader)\n",
        ")\n",
        "\n",
        "# Loss weights (from LeJEPA paper)\n",
        "lambda_contrastive = 1.0\n",
        "lambda_isotropy = 0.01\n",
        "\n",
        "print(\"âœ… Training setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbQLjdVl6hMx"
      },
      "source": [
        "## 7. Training Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4tZW0xc6hMx"
      },
      "outputs": [],
      "source": [
        "def check_isotropy(embeddings, verbose=True):\n",
        "    \"\"\"Verify embeddings are isotropic Gaussian N(0, I)\"\"\"\n",
        "    embeddings = embeddings.detach().cpu()\n",
        "\n",
        "    mean = embeddings.mean(dim=0)\n",
        "    centered = embeddings - mean\n",
        "    cov = (centered.T @ centered) / (embeddings.shape[0] - 1)\n",
        "\n",
        "    mean_norm = torch.norm(mean).item()\n",
        "    cov_error = torch.norm(cov - torch.eye(cov.shape[0]), p='fro').item()\n",
        "    diag_mean = torch.diag(cov).mean().item()\n",
        "\n",
        "    off_diag = cov.clone()\n",
        "    off_diag.fill_diagonal_(0)\n",
        "    off_diag_mean = off_diag.abs().mean().item()\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"  Mean norm: {mean_norm:.4f} (want ~0)\")\n",
        "        print(f\"  Cov error: {cov_error:.4f} (want <5)\")\n",
        "        print(f\"  Diag mean: {diag_mean:.4f} (want ~1)\")\n",
        "        print(f\"  Off-diag: {off_diag_mean:.4f} (want ~0)\")\n",
        "\n",
        "    return {\n",
        "        'mean_norm': mean_norm,\n",
        "        'cov_error': cov_error,\n",
        "        'is_isotropic': mean_norm < 0.5 and cov_error < 10.0\n",
        "    }\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, path):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, path)\n",
        "    print(f\"âœ… Saved: {path}\")\n",
        "\n",
        "print(\"âœ… Utilities defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg5e5h5X6hMx"
      },
      "source": [
        "## 8. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEHz1Jsx6hMx"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, optimizer, scheduler, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_contrastive = 0\n",
        "    total_isotropy = 0\n",
        "\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch in pbar:\n",
        "        # Encode triplets\n",
        "        q_emb = model(batch['queries'])\n",
        "        pos_emb = model(batch['positives'])\n",
        "        neg_emb = model(batch['negatives'])\n",
        "\n",
        "        # Euclidean contrastive loss\n",
        "        pos_dist = torch.norm(q_emb - pos_emb, dim=1)\n",
        "        neg_dist = torch.norm(q_emb - neg_emb, dim=1)\n",
        "        contrastive_loss = torch.relu(pos_dist - neg_dist + 1.0).mean()\n",
        "\n",
        "        # LeJEPA isotropy loss\n",
        "        all_emb = torch.cat([q_emb, pos_emb, neg_emb], dim=0)\n",
        "        isotropy_loss = sigreg(all_emb)\n",
        "\n",
        "        # Combined\n",
        "        loss = lambda_contrastive * contrastive_loss + lambda_isotropy * isotropy_loss\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_contrastive += contrastive_loss.item()\n",
        "        total_isotropy += isotropy_loss.item()\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{loss.item():.4f}',\n",
        "            'contr': f'{contrastive_loss.item():.4f}',\n",
        "            'iso': f'{isotropy_loss.item():.4f}'\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        'loss': total_loss / len(loader),\n",
        "        'contrastive': total_contrastive / len(loader),\n",
        "        'isotropy': total_isotropy / len(loader)\n",
        "    }\n",
        "\n",
        "def validate(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_embeddings = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Validating\"):\n",
        "            q_emb = model(batch['queries'])\n",
        "            pos_emb = model(batch['positives'])\n",
        "            neg_emb = model(batch['negatives'])\n",
        "\n",
        "            pos_dist = torch.norm(q_emb - pos_emb, dim=1)\n",
        "            neg_dist = torch.norm(q_emb - neg_emb, dim=1)\n",
        "            contrastive_loss = torch.relu(pos_dist - neg_dist + 1.0).mean()\n",
        "\n",
        "            all_emb = torch.cat([q_emb, pos_emb, neg_emb], dim=0)\n",
        "            isotropy_loss = sigreg(all_emb)\n",
        "\n",
        "            loss = lambda_contrastive * contrastive_loss + lambda_isotropy * isotropy_loss\n",
        "            total_loss += loss.item()\n",
        "            all_embeddings.append(all_emb)\n",
        "\n",
        "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "    print(\"\\nIsotropy check:\")\n",
        "    isotropy_metrics = check_isotropy(all_embeddings)\n",
        "\n",
        "    return {\n",
        "        'loss': total_loss / len(loader),\n",
        "        'isotropy_metrics': isotropy_metrics\n",
        "    }\n",
        "\n",
        "print(\"âœ… Training functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3LZPE7l6hMx"
      },
      "source": [
        "## 9. Run Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtUT8kzH6hMx"
      },
      "outputs": [],
      "source": [
        "Path('checkpoints').mkdir(exist_ok=True)\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Starting Training!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print('='*60)\n",
        "\n",
        "    # Train\n",
        "    train_metrics = train_epoch(model, train_loader, optimizer, scheduler, epoch)\n",
        "    print(f\"\\nTrain Loss: {train_metrics['loss']:.4f}\")\n",
        "\n",
        "    # Validate\n",
        "    val_metrics = validate(model, val_loader)\n",
        "    print(f\"Val Loss: {val_metrics['loss']:.4f}\")\n",
        "    print(f\"Isotropic: {val_metrics['isotropy_metrics']['is_isotropic']}\")\n",
        "\n",
        "    # Save best\n",
        "    if val_metrics['loss'] < best_val_loss:\n",
        "        best_val_loss = val_metrics['loss']\n",
        "        save_checkpoint(model, optimizer, epoch, 'checkpoints/best_model.pt')\n",
        "        print(\"  â­ New best model!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… Training complete!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8PRFSfU6hMx"
      },
      "source": [
        "## 10. Test Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSCnWgxI6hMy"
      },
      "outputs": [],
      "source": [
        "# Load best model\n",
        "checkpoint = torch.load('checkpoints/best_model.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "# Test\n",
        "test_texts = [\n",
        "    \"What is machine learning?\",\n",
        "    \"Machine learning is a branch of AI\",\n",
        "    \"How to cook pasta\",\n",
        "    \"Python programming tutorial\"\n",
        "]\n",
        "\n",
        "with torch.no_grad():\n",
        "    embeddings = model(test_texts)\n",
        "    print(f\"\\nShape: {embeddings.shape}\")\n",
        "    print(f\"Mean norm: {embeddings.norm(dim=1).mean():.4f}\")\n",
        "    print(f\"Std norm: {embeddings.norm(dim=1).std():.4f}\")\n",
        "\n",
        "check_isotropy(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqc5vmdo6hMy"
      },
      "source": [
        "## 11. Save for RAGCUN\n",
        "\n",
        "Save the trained model to use in the RAGCUN retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVDgbn5W6hMy"
      },
      "outputs": [],
      "source": [
        "# Save final model\n",
        "final_path = 'gaussian_embeddinggemma_final.pt'\n",
        "torch.save(model.state_dict(), final_path)\n",
        "print(f\"âœ… Model saved: {final_path}\")\n",
        "\n",
        "# Download to local machine\n",
        "from google.colab import files\n",
        "files.download(final_path)\n",
        "\n",
        "print(\"\\nðŸ“¥ Download complete!\")\n",
        "print(\"Next step: Add this model to /Users/ctn/src/ctn/ragcun/data/embeddings/\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}