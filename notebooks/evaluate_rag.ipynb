{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation: Original vs LeJEPA Fine-tuned\n",
    "\n",
    "Compare retrieval performance:\n",
    "- **Original**: Cosine similarity on normalized embeddings\n",
    "- **LeJEPA**: Euclidean distance on isotropic Gaussian embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from ragcun import GaussianRetriever\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"✅ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original model (cosine similarity)\n",
    "print(\"Loading original model...\")\n",
    "original_model = SentenceTransformer('google/embeddinggemma-300m', trust_remote_code=True)\n",
    "\n",
    "# LeJEPA model (Euclidean distance)\n",
    "print(\"Loading LeJEPA model...\")\n",
    "lejepa_retriever = GaussianRetriever(\n",
    "    model_path='data/embeddings/gaussian_embeddinggemma_final.pt'\n",
    ")\n",
    "\n",
    "print(\"✅ Models loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Test Dataset\n",
    "\n",
    "We'll use a labeled QA dataset to measure retrieval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load MS MARCO dev set (query-doc pairs)\n",
    "print(\"Loading test data...\")\n",
    "dataset = load_dataset('sentence-transformers/msmarco-hard-negatives', 'triplet', split='train', streaming=True)\n",
    "\n",
    "# Take 500 examples\n",
    "test_data = []\n",
    "for i, example in enumerate(dataset):\n",
    "    if i >= 500:\n",
    "        break\n",
    "    test_data.append({\n",
    "        'query': example['query'],\n",
    "        'positive': example['positive'],\n",
    "        'negative': example['negative']\n",
    "    })\n",
    "\n",
    "print(f\"✅ Loaded {len(test_data)} test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Document Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all documents (positives + negatives)\n",
    "all_docs = []\n",
    "query_to_correct_doc_idx = {}  # Map query index to correct doc index\n",
    "\n",
    "for i, example in enumerate(test_data):\n",
    "    # Add positive doc\n",
    "    pos_idx = len(all_docs)\n",
    "    all_docs.append(example['positive'])\n",
    "    query_to_correct_doc_idx[i] = pos_idx\n",
    "    \n",
    "    # Add negative doc\n",
    "    all_docs.append(example['negative'])\n",
    "\n",
    "print(f\"Total documents: {len(all_docs)}\")\n",
    "print(f\"Queries: {len(test_data)}\")\n",
    "\n",
    "# Add to LeJEPA retriever\n",
    "print(\"\\nIndexing with LeJEPA...\")\n",
    "lejepa_retriever.add_documents(all_docs, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all docs with original model\n",
    "print(\"Encoding with original model...\")\n",
    "original_doc_embs = original_model.encode(\n",
    "    all_docs,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True  # Normalize for cosine\n",
    ")\n",
    "\n",
    "print(f\"✅ Original embeddings shape: {original_doc_embs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Retrieval Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(model, queries, doc_embs, query_to_correct, method='cosine', top_k=10):\n",
    "    \"\"\"\n",
    "    Evaluate retrieval performance.\n",
    "    \n",
    "    Returns:\n",
    "        - Recall@k\n",
    "        - MRR (Mean Reciprocal Rank)\n",
    "        - Separation (avg distance to pos vs neg)\n",
    "    \"\"\"\n",
    "    recalls = []\n",
    "    reciprocal_ranks = []\n",
    "    pos_scores = []\n",
    "    neg_scores = []\n",
    "    \n",
    "    # Encode queries\n",
    "    if method == 'cosine':\n",
    "        query_embs = model.encode(queries, show_progress_bar=True, normalize_embeddings=True)\n",
    "    \n",
    "    for i, query in enumerate(queries):\n",
    "        correct_idx = query_to_correct[i]\n",
    "        \n",
    "        if method == 'cosine':\n",
    "            # Cosine similarity\n",
    "            query_emb = query_embs[i:i+1]\n",
    "            similarities = cosine_similarity(query_emb, doc_embs)[0]\n",
    "            \n",
    "            # Rank by similarity (higher = better)\n",
    "            ranked_indices = np.argsort(-similarities)\n",
    "            \n",
    "            # Scores for pos/neg\n",
    "            pos_scores.append(similarities[correct_idx])\n",
    "            neg_indices = [j for j in range(len(doc_embs)) if j != correct_idx]\n",
    "            neg_scores.append(similarities[neg_indices].mean())\n",
    "        else:\n",
    "            # Euclidean distance (LeJEPA)\n",
    "            results = lejepa_retriever.retrieve(query, top_k=top_k)\n",
    "            ranked_indices = [all_docs.index(doc) for doc, _ in results]\n",
    "            distances = [dist for _, dist in results]\n",
    "            \n",
    "            # Scores (lower = better for distance)\n",
    "            correct_doc = all_docs[correct_idx]\n",
    "            if correct_doc in [doc for doc, _ in results]:\n",
    "                pos_idx = [doc for doc, _ in results].index(correct_doc)\n",
    "                pos_scores.append(-distances[pos_idx])  # Negative for comparison\n",
    "            else:\n",
    "                # Not in top-k, estimate\n",
    "                pos_scores.append(-100)  # Penalty\n",
    "            \n",
    "            neg_scores.append(-np.mean([d for d in distances if d != distances[pos_idx] if 'pos_idx' in locals()]))\n",
    "        \n",
    "        # Recall@k\n",
    "        recall = 1 if correct_idx in ranked_indices[:top_k] else 0\n",
    "        recalls.append(recall)\n",
    "        \n",
    "        # MRR\n",
    "        if correct_idx in ranked_indices[:top_k]:\n",
    "            rank = list(ranked_indices[:top_k]).index(correct_idx) + 1\n",
    "            reciprocal_ranks.append(1.0 / rank)\n",
    "        else:\n",
    "            reciprocal_ranks.append(0.0)\n",
    "    \n",
    "    return {\n",
    "        'recall@k': np.mean(recalls),\n",
    "        'mrr': np.mean(reciprocal_ranks),\n",
    "        'avg_pos_score': np.mean(pos_scores),\n",
    "        'avg_neg_score': np.mean(neg_scores),\n",
    "        'separation': np.mean(pos_scores) - np.mean(neg_scores)\n",
    "    }\n",
    "\n",
    "# Extract queries\n",
    "queries = [ex['query'] for ex in test_data]\n",
    "\n",
    "print(\"\\nEvaluating Original (Cosine Similarity)...\")\n",
    "original_results = evaluate_retrieval(\n",
    "    original_model, queries, original_doc_embs, \n",
    "    query_to_correct_doc_idx, method='cosine', top_k=10\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluating LeJEPA (Euclidean Distance)...\")\n",
    "lejepa_results = evaluate_retrieval(\n",
    "    None, queries, None,\n",
    "    query_to_correct_doc_idx, method='euclidean', top_k=10\n",
    ")\n",
    "\n",
    "print(\"✅ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['Recall@10', 'MRR', 'Avg Pos Score', 'Avg Neg Score', 'Separation'],\n",
    "    'Original (Cosine)': [\n",
    "        f\"{original_results['recall@k']:.4f}\",\n",
    "        f\"{original_results['mrr']:.4f}\",\n",
    "        f\"{original_results['avg_pos_score']:.4f}\",\n",
    "        f\"{original_results['avg_neg_score']:.4f}\",\n",
    "        f\"{original_results['separation']:.4f}\"\n",
    "    ],\n",
    "    'LeJEPA (Euclidean)': [\n",
    "        f\"{lejepa_results['recall@k']:.4f}\",\n",
    "        f\"{lejepa_results['mrr']:.4f}\",\n",
    "        f\"{lejepa_results['avg_pos_score']:.4f}\",\n",
    "        f\"{lejepa_results['avg_neg_score']:.4f}\",\n",
    "        f\"{lejepa_results['separation']:.4f}\"\n",
    "    ],\n",
    "    'Improvement': [\n",
    "        f\"{(lejepa_results['recall@k'] - original_results['recall@k']):.4f}\",\n",
    "        f\"{(lejepa_results['mrr'] - original_results['mrr']):.4f}\",\n",
    "        f\"-\",\n",
    "        f\"-\",\n",
    "        f\"{(lejepa_results['separation'] - original_results['separation']):.4f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RAG RETRIEVAL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save\n",
    "comparison.to_csv('data/processed/rag_performance_comparison.csv', index=False)\n",
    "print(\"\\n✅ Saved to data/processed/rag_performance_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Recall comparison\n",
    "metrics = ['Recall@10', 'MRR']\n",
    "original_vals = [original_results['recall@k'], original_results['mrr']]\n",
    "lejepa_vals = [lejepa_results['recall@k'], lejepa_results['mrr']]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, original_vals, width, label='Original (Cosine)', alpha=0.8)\n",
    "axes[0].bar(x + width/2, lejepa_vals, width, label='LeJEPA (Euclidean)', alpha=0.8)\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Retrieval Performance')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(metrics)\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Separation comparison\n",
    "separations = [original_results['separation'], lejepa_results['separation']]\n",
    "models = ['Original\\n(Cosine)', 'LeJEPA\\n(Euclidean)']\n",
    "\n",
    "axes[1].bar(models, separations, alpha=0.8, color=['blue', 'green'])\n",
    "axes[1].set_ylabel('Pos-Neg Separation')\n",
    "axes[1].set_title('Score Separation (Higher = Better)')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/processed/rag_performance.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Visualization saved to data/processed/rag_performance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "1. **Recall@10**: How often is the correct document in top 10?\n",
    "2. **MRR**: Average reciprocal rank (1/position of correct doc)\n",
    "3. **Separation**: Distance between positive and negative scores\n",
    "\n",
    "**Expected Results:**\n",
    "- LeJEPA should show **higher separation** (better discrimination)\n",
    "- Similar or better Recall/MRR\n",
    "- More calibrated scores for downstream ranking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
    "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
