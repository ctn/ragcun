# ðŸš€ Quick Start: Publication-Ready RAG Training

**All Phase 1 features implemented and tested!** âœ…

---

## âš¡ 5-Minute Quick Start

```bash
# 1. Download MS MARCO (10K subset for quick testing)
python scripts/download_msmarco.py \
  --output_dir data/processed/msmarco_10k \
  --max_train_samples 10000

# 2. Train (Smart Hybrid - fast!)
./scripts/train_smart_hybrid.sh \
  data/processed/msmarco_10k/train.json \
  data/processed/msmarco_10k/dev.json \
  checkpoints/quick_test \
  1

# 3. Evaluate on BEIR
python scripts/evaluate_beir.py \
  --model_path checkpoints/quick_test/best_model.pt \
  --datasets scifact nfcorpus
```

**Time:** ~30 min total  
**Cost:** ~$0.50 on spot instance

---

## ðŸ“Š Full Pipeline (Publication Quality)

```bash
# 1. Download full MS MARCO (~30 min)
python scripts/download_msmarco.py \
  --output_dir data/processed/msmarco

# 2. Train (2-3 hours on V100, 45 min on 8xA100)
./scripts/train_smart_hybrid.sh \
  data/processed/msmarco/train.json \
  data/processed/msmarco/dev.json \
  checkpoints/smart_hybrid \
  3

# 3. Evaluate on BEIR standard set (~30 min)
python scripts/evaluate_beir.py \
  --model_path checkpoints/smart_hybrid/best_model.pt \
  --datasets scifact nfcorpus arguana fiqa trec-covid \
  --output_file results/beir_results.json
```

**Time:** ~3-4 hours total  
**Cost:** ~$10-20 on spot instance

---

## ðŸ§ª Test Everything

```bash
# Run all 36 unit tests
pytest tests/test_model_smart_hybrid.py \
       tests/test_download_scripts.py \
       tests/test_beir_evaluation.py -v

# Expected: 36 passed âœ…
```

---

## ðŸ“ What Was Implemented

### âœ… New Scripts (5)
1. `scripts/download_msmarco.py` - Download MS MARCO dataset
2. `scripts/download_wiki.py` - Download Wikipedia for unsupervised training
3. `scripts/evaluate_beir.py` - Evaluate on BEIR benchmark
4. `scripts/train_smart_hybrid.sh` - Convenient training wrapper
5. `scripts/train_updates.py` - Reference for DDP updates

### âœ… Updated Core Files (2)
1. `ragcun/model.py` - Smart hybrid training support
2. `scripts/train.py` - Multi-GPU + differential learning rates

### âœ… New Tests (3 files, 36 tests)
1. `tests/test_model_smart_hybrid.py` - 11 tests
2. `tests/test_download_scripts.py` - 13 tests
3. `tests/test_beir_evaluation.py` - 12 tests

### âœ… Updated Tests (1)
1. `tests/conftest.py` - Enhanced mock infrastructure

---

## ðŸŽ¯ Key Features

### Model
- âœ… Configurable base models (any SentenceTransformer)
- âœ… Freeze base encoder (train projection only)
- âœ… Dynamic embedding dimensions
- âœ… Parameter group extraction

### Training
- âœ… Multi-GPU support (PyTorch DDP)
- âœ… Differential learning rates (base vs projection)
- âœ… Distributed data loading
- âœ… Automatic device placement

### Evaluation
- âœ… BEIR benchmark (15+ datasets)
- âœ… Comprehensive metrics (MRR, NDCG, Recall, MAP, Precision)
- âœ… Auto-downloading datasets
- âœ… Efficient batch encoding

### Data
- âœ… MS MARCO downloader with formatting
- âœ… Wikipedia downloader for unsupervised training
- âœ… Configurable dataset sizes
- âœ… Automatic validation

---

## ðŸ’¡ Training Strategies

### Strategy 1: Smart Hybrid (Recommended)
**Best for:** First publication, quick experiments  
**Time:** 2-3 hours  
**Params:** 1.2M trainable (projection only)

```bash
./scripts/train_smart_hybrid.sh \
  data/processed/msmarco/train.json \
  data/processed/msmarco/dev.json \
  checkpoints/smart_hybrid \
  3
```

### Strategy 2: Full Fine-Tuning
**Best for:** Maximum performance  
**Time:** 8-12 hours  
**Params:** 300M trainable (full model)

```bash
python scripts/train.py \
  --train_data data/processed/msmarco/train.json \
  --val_data data/processed/msmarco/dev.json \
  --base_model google/embeddinggemma-300m \
  --freeze_base False \
  --base_learning_rate 1e-5 \
  --projection_learning_rate 5e-4 \
  --epochs 3 \
  --output_dir checkpoints/full_finetune
```

### Strategy 3: Unsupervised + Supervised
**Best for:** Architecture novelty claims  
**Time:** 5-7 hours total  

```bash
# Unsupervised pre-training
python scripts/download_wiki.py --num_passages 100000
python scripts/train.py \
  --train_data data/processed/wiki/train.json \
  --freeze_base True \
  --epochs 3 \
  --output_dir checkpoints/unsupervised

# Supervised fine-tuning
python scripts/train.py \
  --train_data data/processed/msmarco/train.json \
  --resume checkpoints/unsupervised/best_model.pt \
  --base_learning_rate 1e-5 \
  --epochs 3 \
  --output_dir checkpoints/finetuned
```

---

## ðŸ”¥ Multi-GPU Training

```bash
# Automatic (detects #GPUs)
./scripts/train_smart_hybrid.sh ...

# Manual (4 GPUs)
torchrun --nproc_per_node=4 scripts/train.py \
  --train_data data/processed/msmarco/train.json \
  --base_model sentence-transformers/all-mpnet-base-v2 \
  --freeze_base \
  --projection_learning_rate 5e-4 \
  --batch_size 32 \
  --output_dir checkpoints/multi_gpu
```

---

## ðŸ“ˆ Expected Performance

### Smart Hybrid (MPNet base + trained projection)
- **Training Time:** 2-3 hours (V100), 45 min (8xA100)
- **BEIR Average NDCG@10:** ~0.45-0.50 (competitive)
- **Trainable Params:** 1.2M (0.4% of full model)
- **Cost:** $6-15

### Full Fine-Tuning (EmbeddingGemma)
- **Training Time:** 8-12 hours (V100), 2-3 hours (8xA100)
- **BEIR Average NDCG@10:** ~0.50-0.55 (SOTA-competitive)
- **Trainable Params:** 300M (100%)
- **Cost:** $20-60

---

## ðŸ› Troubleshooting

### Issue: "datasets module not found"
```bash
pip install datasets tqdm
```

### Issue: "HuggingFace 403 Forbidden"
```bash
# Add to .env file
echo "HF_TOKEN=your_token_here" >> .env

# Accept model terms on HuggingFace website
# Enable "Access to public gated repositories" in token settings
```

### Issue: "CUDA out of memory"
```bash
# Reduce batch size
python scripts/train.py --batch_size 8  # Instead of 16

# Or use gradient accumulation
python scripts/train.py \
  --batch_size 8 \
  --gradient_accumulation_steps 2  # Effective batch = 16
```

### Issue: "BEIR dataset download slow"
Datasets cache in `data/beir/` after first download. Subsequent evaluations are fast.

---

## ðŸ“š Documentation

- **Full Implementation Details:** `PHASE1_IMPLEMENTATION_SUMMARY.md`
- **Training Guide:** `docs/PUBLICATION_TRAINING_GUIDE.md`
- **Implementation Recipes:** `docs/PUBLICATION_TRAINING_IMPLEMENTATION.md`

---

## âœ… Verification Checklist

- [ ] All tests pass: `pytest tests/test_*.py -v`
- [ ] Can download MS MARCO: `python scripts/download_msmarco.py --max_train_samples 100 --output_dir /tmp/test`
- [ ] Can train (quick): Run 1 epoch on 100 samples
- [ ] Can evaluate: `python scripts/evaluate_beir.py` on trained model
- [ ] Multi-GPU works: `torchrun --nproc_per_node=2 scripts/train.py ...`

---

**Status:** âœ… Phase 1 Complete  
**Tests:** 36/36 passing  
**Ready for:** Full training runs and paper writing

ðŸŽ‰ **Happy Training!**

